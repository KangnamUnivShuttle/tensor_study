{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Embedding\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils, to_categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면', '두개랑', '탕수육', '작은걸로', '갖다주세요']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '짜장면 두개랑 탕수육 작은걸로 갖다주세요'\n",
    "text_to_word_sequence(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words\n",
    "\n",
    "- 단어별로 나누어서 가방에 넣어둔 뒤 얼마나 많은 빈도수로 쓰였는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    '2020년까지 많은 데이터가 구축되었다.',\n",
    "    '그 중에서 비교적 대부분의 사람들이 접근할 수 있는 오픈 데이터를 정리하였다.',\n",
    "    '구할 수 있는 모든 데이터를 쏟아 부어서 end to end로 모델을 만들어 보겠다는 포부를 가진 분들의 진입을 쉽게하기 위한 목적이고, 정교한 데이터 구축을 위해서는 이후에 어떠한 데이터가 필요한지를 살펴보기 위한 과정이다.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('2020년까지', 1),\n",
       "             ('많은', 1),\n",
       "             ('데이터가', 2),\n",
       "             ('구축되었다', 1),\n",
       "             ('그', 1),\n",
       "             ('중에서', 1),\n",
       "             ('비교적', 1),\n",
       "             ('대부분의', 1),\n",
       "             ('사람들이', 1),\n",
       "             ('접근할', 1),\n",
       "             ('수', 2),\n",
       "             ('있는', 2),\n",
       "             ('오픈', 1),\n",
       "             ('데이터를', 2),\n",
       "             ('정리하였다', 1),\n",
       "             ('구할', 1),\n",
       "             ('모든', 1),\n",
       "             ('쏟아', 1),\n",
       "             ('부어서', 1),\n",
       "             ('end', 1),\n",
       "             ('to', 1),\n",
       "             ('end로', 1),\n",
       "             ('모델을', 1),\n",
       "             ('만들어', 1),\n",
       "             ('보겠다는', 1),\n",
       "             ('포부를', 1),\n",
       "             ('가진', 1),\n",
       "             ('분들의', 1),\n",
       "             ('진입을', 1),\n",
       "             ('쉽게하기', 1),\n",
       "             ('위한', 2),\n",
       "             ('목적이고', 1),\n",
       "             ('정교한', 1),\n",
       "             ('데이터', 1),\n",
       "             ('구축을', 1),\n",
       "             ('위해서는', 1),\n",
       "             ('이후에', 1),\n",
       "             ('어떠한', 1),\n",
       "             ('필요한지를', 1),\n",
       "             ('살펴보기', 1),\n",
       "             ('과정이다', 1)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "token.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.document_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document_count 는 총 몇 개의 문장인지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'데이터가': 2,\n",
       "             '많은': 1,\n",
       "             '2020년까지': 1,\n",
       "             '구축되었다': 1,\n",
       "             '대부분의': 1,\n",
       "             '중에서': 1,\n",
       "             '오픈': 1,\n",
       "             '사람들이': 1,\n",
       "             '데이터를': 2,\n",
       "             '접근할': 1,\n",
       "             '있는': 2,\n",
       "             '정리하였다': 1,\n",
       "             '그': 1,\n",
       "             '비교적': 1,\n",
       "             '수': 2,\n",
       "             '데이터': 1,\n",
       "             '가진': 1,\n",
       "             'end로': 1,\n",
       "             '보겠다는': 1,\n",
       "             '모델을': 1,\n",
       "             '포부를': 1,\n",
       "             '분들의': 1,\n",
       "             '위한': 1,\n",
       "             '필요한지를': 1,\n",
       "             '정교한': 1,\n",
       "             '만들어': 1,\n",
       "             '살펴보기': 1,\n",
       "             '진입을': 1,\n",
       "             '이후에': 1,\n",
       "             '위해서는': 1,\n",
       "             '구할': 1,\n",
       "             '구축을': 1,\n",
       "             '목적이고': 1,\n",
       "             'end': 1,\n",
       "             '모든': 1,\n",
       "             '쉽게하기': 1,\n",
       "             '과정이다': 1,\n",
       "             '쏟아': 1,\n",
       "             'to': 1,\n",
       "             '부어서': 1,\n",
       "             '어떠한': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.word_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 단어들이 몇 개의 문장에서 나오는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    '2020년까지 많은 데이터가 구축되었다. 그 중에서 비교적 대부분의 사람들이 접근할 수 있는 오픈 데이터를 정리하였다. 구할 수 있는 모든 데이터를 쏟아 부어서 end to end로 모델을 만들어 보겠다는 포부를 가진 분들의 진입을 쉽게하기 위한 목적이고, 정교한 데이터 구축을 위해서는 이후에 어떠한 데이터가 필요한지를 살펴보기 위한 과정이다.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('2020년까지', 1),\n",
       "             ('많은', 1),\n",
       "             ('데이터가', 2),\n",
       "             ('구축되었다', 1),\n",
       "             ('그', 1),\n",
       "             ('중에서', 1),\n",
       "             ('비교적', 1),\n",
       "             ('대부분의', 1),\n",
       "             ('사람들이', 1),\n",
       "             ('접근할', 1),\n",
       "             ('수', 2),\n",
       "             ('있는', 2),\n",
       "             ('오픈', 1),\n",
       "             ('데이터를', 2),\n",
       "             ('정리하였다', 1),\n",
       "             ('구할', 1),\n",
       "             ('모든', 1),\n",
       "             ('쏟아', 1),\n",
       "             ('부어서', 1),\n",
       "             ('end', 1),\n",
       "             ('to', 1),\n",
       "             ('end로', 1),\n",
       "             ('모델을', 1),\n",
       "             ('만들어', 1),\n",
       "             ('보겠다는', 1),\n",
       "             ('포부를', 1),\n",
       "             ('가진', 1),\n",
       "             ('분들의', 1),\n",
       "             ('진입을', 1),\n",
       "             ('쉽게하기', 1),\n",
       "             ('위한', 2),\n",
       "             ('목적이고', 1),\n",
       "             ('정교한', 1),\n",
       "             ('데이터', 1),\n",
       "             ('구축을', 1),\n",
       "             ('위해서는', 1),\n",
       "             ('이후에', 1),\n",
       "             ('어떠한', 1),\n",
       "             ('필요한지를', 1),\n",
       "             ('살펴보기', 1),\n",
       "             ('과정이다', 1)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "token.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'데이터': 1,\n",
       "             '중에서': 1,\n",
       "             '구축되었다': 1,\n",
       "             'end로': 1,\n",
       "             '보겠다는': 1,\n",
       "             '가진': 1,\n",
       "             '있는': 1,\n",
       "             '모델을': 1,\n",
       "             '수': 1,\n",
       "             '포부를': 1,\n",
       "             '분들의': 1,\n",
       "             '위한': 1,\n",
       "             '필요한지를': 1,\n",
       "             '정교한': 1,\n",
       "             '정리하였다': 1,\n",
       "             '만들어': 1,\n",
       "             '데이터를': 1,\n",
       "             '살펴보기': 1,\n",
       "             '진입을': 1,\n",
       "             '이후에': 1,\n",
       "             '위해서는': 1,\n",
       "             '대부분의': 1,\n",
       "             '많은': 1,\n",
       "             '2020년까지': 1,\n",
       "             '구할': 1,\n",
       "             '구축을': 1,\n",
       "             '접근할': 1,\n",
       "             '목적이고': 1,\n",
       "             '그': 1,\n",
       "             '비교적': 1,\n",
       "             'end': 1,\n",
       "             '오픈': 1,\n",
       "             '모든': 1,\n",
       "             '사람들이': 1,\n",
       "             '쉽게하기': 1,\n",
       "             '과정이다': 1,\n",
       "             '쏟아': 1,\n",
       "             'to': 1,\n",
       "             '데이터가': 1,\n",
       "             '부어서': 1,\n",
       "             '어떠한': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.word_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자열을 일부러 합쳐서 하나의 문자열로 바꾸었지만 . 로 문장 끝은 알려준 상태\n",
    "\n",
    "하지만 각 문자열 갯수를 문장으로 인식하는 것으로 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'데이터가': 1,\n",
       " '수': 2,\n",
       " '있는': 3,\n",
       " '데이터를': 4,\n",
       " '위한': 5,\n",
       " '2020년까지': 6,\n",
       " '많은': 7,\n",
       " '구축되었다': 8,\n",
       " '그': 9,\n",
       " '중에서': 10,\n",
       " '비교적': 11,\n",
       " '대부분의': 12,\n",
       " '사람들이': 13,\n",
       " '접근할': 14,\n",
       " '오픈': 15,\n",
       " '정리하였다': 16,\n",
       " '구할': 17,\n",
       " '모든': 18,\n",
       " '쏟아': 19,\n",
       " '부어서': 20,\n",
       " 'end': 21,\n",
       " 'to': 22,\n",
       " 'end로': 23,\n",
       " '모델을': 24,\n",
       " '만들어': 25,\n",
       " '보겠다는': 26,\n",
       " '포부를': 27,\n",
       " '가진': 28,\n",
       " '분들의': 29,\n",
       " '진입을': 30,\n",
       " '쉽게하기': 31,\n",
       " '목적이고': 32,\n",
       " '정교한': 33,\n",
       " '데이터': 34,\n",
       " '구축을': 35,\n",
       " '위해서는': 36,\n",
       " '이후에': 37,\n",
       " '어떠한': 38,\n",
       " '필요한지를': 39,\n",
       " '살펴보기': 40,\n",
       " '과정이다': 41}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 데이터가 존재하는 순서 인덱스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word one-hot-coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면', '두개랑', '탕수육', '작은걸로', '갖다주세요']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '짜장면 두개랑 탕수육 작은걸로 갖다주세요'\n",
    "text_to_word_sequence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'짜장면': 1, '두개랑': 2, '탕수육': 3, '작은걸로': 4, '갖다주세요': 5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts([text])\n",
    "token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.texts_to_sequences([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_size = len(token.word_index) + 1 # 배열이 0부터 시작하니 일부러 1번지 부터 시작하기 위해 사이즈를 늘림\n",
    "to_categorical(token.texts_to_sequences([text]), num_classes=word_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "- 위와 같이 단어들을 one-hot-coding 하면 벡터 길이가 n 개로 너무 길어짐\n",
    "- n 길이의 벡터를 정해진 사이즈로 줄임\n",
    "- 어떤 단어가 다른 단어와 유사하다 라는 정보를 이용해 벡터 값을 고려한 one-hot-coding\n",
    "\n",
    "ex)\n",
    "\n",
    "행복함 은 좋음 이란 단어와 가깝지만 나쁨 단어와는 거리가 있다 이런 정보\n",
    "\n",
    "Dense Representation 밀집 표현이라고도 부름\n",
    "\n",
    "```\n",
    "[0., 0., 0., 0., 0., 1.]\n",
    "```\n",
    "\n",
    "위가 희소 표현 방식이라면\n",
    "\n",
    "```\n",
    "[0.24, 0.113, 0.2541, 0.2566]\n",
    "```\n",
    "\n",
    "위가 밀집 표현 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "<img width=\"715\" alt=\"스크린샷 2021-12-02 오후 4 23 15\" src=\"https://user-images.githubusercontent.com/16532326/144376426-ba1fa8ec-14c9-4d1d-af50-e24b6e09f174.png\">\n",
    "\n",
    "\n",
    "[데모](http://w.elnn.kr/search/) 사이트에 내용을 보면 \n",
    "\n",
    "입력 데이터 벡터의 유사도를 이용하여 결과를 만들어 낼 수 있는 걸 알 수 있음\n",
    "\n",
    "- 단어의 의미를 벡터로 나타내는게 분산 표현\n",
    "- 단어의 유사도를 벡터로 나타내는게 워드 임베딩\n",
    "- 이를 저차원의 벡터로 변환한게  밀집 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분산 표현(Distributed Representation)\n",
    "\n",
    "분포 가설을 이용\n",
    "\n",
    "```\n",
    "비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다\n",
    "```\n",
    "\n",
    "저차원의 벡터에 여러차원에 걸쳐 단어의 의미를 분산 시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW(Continuous Bag of Words)\n",
    "\n",
    "_현재 포인팅하고 있는 위치의 주변에 있는 단어들을 이용하여 현재 포인팅하는 위치의 단어를 예측_\n",
    "\n",
    "ex)\n",
    "\n",
    "This is my dog\n",
    "\n",
    "This <span style=\"color:blue\"> **is** </span> <span style=\"color:red\"> **??** </span> <span style=\"color:blue\"> **dog** </span>\n",
    "\n",
    "위 경우 처럼 `??` 라는 단어 주변에 있는 `is`, `dog` 를 이용하여 `??` 를 예측하는것\n",
    "\n",
    "용어로 예측할 가운데 단어를 center word, 주변 단어를 context word 라고 부름\n",
    "\n",
    "context word 의 길이를 window 라고 부르며 위의 경우 1\n",
    "\n",
    "![asdf](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)\n",
    "\n",
    "위 그림을 보면 중심 단어를 계속 이동시키며 학습을 하는데 이를 슬라이딩 윈도우 라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습은 주변단어를 입력, 중심단어를 출력으로 지정하게 됨\n",
    "\n",
    "따라서 중심 단어를 잘 예측하려면 중심 단어 one-hot-vector 필요\n",
    "\n",
    "`CBOW` 는 레이어가 하나인 얕은 신경망 구조와 활성함수가 없는 룩업 테이블 구조이기 때문에 프로젝션 레이어라고도 부름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![asdf](https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG)\n",
    "\n",
    "Matrix: 5, Word vector: 7 (The, fat, cat, sat, on, the, mat)\n",
    "\n",
    "W 행렬은 기본 랜덤 값으로 정해져 있음\n",
    "\n",
    "여기다 x 값으로 cat 이 들어온다면\n",
    "\n",
    "```\n",
    "[0, 0, 1, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "값이 채워진 걸과 W 행렬이 곱해질 것 이고, 사실상 결과 값으로 나올 V (cat) 값은 \n",
    "\n",
    "W 행렬의 x 값이 있었던 행의 값만 나오게 됨\n",
    "\n",
    "위 작업이 룩업 테이블 작업이고 이 데이터가 사실상 M 차원의 임베딩 데이터\n",
    "\n",
    "![asdf](https://wikidocs.net/images/page/22660/word2vec_renew_4.PNG)\n",
    "\n",
    "전 작업을 만약 윈도우 사이즈가 2 == n 라면 2 * n 번 각각 진행 하고,\n",
    "\n",
    "그의 벡터 평균을 계산함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![asdf](https://wikidocs.net/images/page/22660/word2vec_renew_5.PNG)\n",
    "\n",
    "전 결과로 벡터 평균 v 를 이용하여\n",
    "\n",
    "출력 층에도 있는 W' 에 값을 곱함\n",
    "\n",
    "그럼 다음 softmax 를 이용해 해당 벡터가 어느 위치에 있을 확률이 높은지를 계산함\n",
    "\n",
    "그럼다음 실제 y 값과 유사하게 만들기 위해 손실 항수로 cross-entropy 를 사용함 \n",
    "\n",
    "![asdf](https://wikidocs.net/images/page/22660/crossentrophy.PNG)\n",
    "\n",
    "쉽게 말해 -log 함수를 사용하여 모양이 대략\n",
    "\n",
    "<img width=\"238\" alt=\"스크린샷 2021-12-02 오후 5 36 50\" src=\"https://user-images.githubusercontent.com/16532326/144386627-43c16337-05d7-4add-a1e0-775fc1340ab9.png\">\n",
    "\n",
    "위와 같이 생기며, 출력 층의 one-hot-coding 된 y 벡터의 값이 있고, softmax(W' x v) 결과 간의 오차가 작아지게\n",
    "\n",
    "만들기 위해 역전파를 사용하여 수정을 해감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그럼 단어 유사도 계산 방법은?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Embedding(16, 4, input_length=2)\n",
    "```\n",
    "\n",
    "16개의 단어가 입력으로 들어오지만 벡터 크기는 4로 고정하고, 입력은 매번 단어 2개씩만 들어간다 라는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 긍/부정 판별\n",
    "\n",
    "[테스트 데이터](https://movie.naver.com/movie/bi/mi/point.naver?code=10016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    '크리스마스 최고의 영화',\n",
    "    '봐도봐도 질리지 않네',\n",
    "    '내인생 최고의 영화~!!',\n",
    "    '역사상 최고의 크리스마스 영화. 영화음악이 명곡이다.',\n",
    "    '언제 봐도 재미가 있는 영화여서 많이본다',\n",
    "    '유치찬란 허무맹랑 어의상실',\n",
    "    '으엑 이게뭐얔 무슨영화야 ㅋ',\n",
    "    '별로... 별로...',\n",
    "    '명절날 지겹게 봐서 1점',\n",
    "    '재미있어도 이제 그만 보고 싶음.. 너무 질려서'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_class = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'최고의': 1,\n",
       "  '영화': 2,\n",
       "  '크리스마스': 3,\n",
       "  '별로': 4,\n",
       "  '봐도봐도': 5,\n",
       "  '질리지': 6,\n",
       "  '않네': 7,\n",
       "  '내인생': 8,\n",
       "  '역사상': 9,\n",
       "  '영화음악이': 10,\n",
       "  '명곡이다': 11,\n",
       "  '언제': 12,\n",
       "  '봐도': 13,\n",
       "  '재미가': 14,\n",
       "  '있는': 15,\n",
       "  '영화여서': 16,\n",
       "  '많이본다': 17,\n",
       "  '유치찬란': 18,\n",
       "  '허무맹랑': 19,\n",
       "  '어의상실': 20,\n",
       "  '으엑': 21,\n",
       "  '이게뭐얔': 22,\n",
       "  '무슨영화야': 23,\n",
       "  'ㅋ': 24,\n",
       "  '명절날': 25,\n",
       "  '지겹게': 26,\n",
       "  '봐서': 27,\n",
       "  '1점': 28,\n",
       "  '재미있어도': 29,\n",
       "  '이제': 30,\n",
       "  '그만': 31,\n",
       "  '보고': 32,\n",
       "  '싶음': 33,\n",
       "  '너무': 34,\n",
       "  '질려서': 35},\n",
       " OrderedDict([('크리스마스', 2),\n",
       "              ('최고의', 3),\n",
       "              ('영화', 3),\n",
       "              ('봐도봐도', 1),\n",
       "              ('질리지', 1),\n",
       "              ('않네', 1),\n",
       "              ('내인생', 1),\n",
       "              ('역사상', 1),\n",
       "              ('영화음악이', 1),\n",
       "              ('명곡이다', 1),\n",
       "              ('언제', 1),\n",
       "              ('봐도', 1),\n",
       "              ('재미가', 1),\n",
       "              ('있는', 1),\n",
       "              ('영화여서', 1),\n",
       "              ('많이본다', 1),\n",
       "              ('유치찬란', 1),\n",
       "              ('허무맹랑', 1),\n",
       "              ('어의상실', 1),\n",
       "              ('으엑', 1),\n",
       "              ('이게뭐얔', 1),\n",
       "              ('무슨영화야', 1),\n",
       "              ('ㅋ', 1),\n",
       "              ('별로', 2),\n",
       "              ('명절날', 1),\n",
       "              ('지겹게', 1),\n",
       "              ('봐서', 1),\n",
       "              ('1점', 1),\n",
       "              ('재미있어도', 1),\n",
       "              ('이제', 1),\n",
       "              ('그만', 1),\n",
       "              ('보고', 1),\n",
       "              ('싶음', 1),\n",
       "              ('너무', 1),\n",
       "              ('질려서', 1)]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "token.word_index, token.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 2],\n",
       " [5, 6, 7],\n",
       " [8, 1, 2],\n",
       " [9, 1, 3, 2, 10, 11],\n",
       " [12, 13, 14, 15, 16, 17],\n",
       " [18, 19, 20],\n",
       " [21, 22, 23, 24],\n",
       " [4, 4],\n",
       " [25, 26, 27, 28],\n",
       " [29, 30, 31, 32, 33, 34, 35]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = token.texts_to_sequences(docs)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 벡터들의 길이가 서로 다 다르니 맞쳐주기 위해 패딩 기법사용하여\n",
    "\n",
    "길이들을 다 통일 시킴\n",
    "\n",
    "정해진 길이보다 길면 잘라냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  3,  1,  2],\n",
       "       [ 0,  5,  6,  7],\n",
       "       [ 0,  8,  1,  2],\n",
       "       [ 3,  2, 10, 11],\n",
       "       [14, 15, 16, 17],\n",
       "       [ 0, 18, 19, 20],\n",
       "       [21, 22, 23, 24],\n",
       "       [ 0,  0,  4,  4],\n",
       "       [25, 26, 27, 28],\n",
       "       [32, 33, 34, 35]], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x = pad_sequences(x, 4)\n",
    "padded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_size = len(token.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x7fdf81f45588>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding(word_size, 8, input_length=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4개의 패딩을 만들었으므로 input_length = 4,\n",
    "\n",
    "임의로 8길이의 벡터를 출력으로 지정,\n",
    "\n",
    "word_size 는 총 입력 단어의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 8)              288       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 321\n",
      "Trainable params: 321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Embedding(word_size, 8, input_length=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 1s 60ms/sample - loss: 0.6910 - accuracy: 0.6000\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 320us/sample - loss: 0.6885 - accuracy: 0.6000\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 373us/sample - loss: 0.6860 - accuracy: 0.6000\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 310us/sample - loss: 0.6834 - accuracy: 0.6000\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 275us/sample - loss: 0.6809 - accuracy: 0.6000\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 252us/sample - loss: 0.6784 - accuracy: 0.7000\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 305us/sample - loss: 0.6758 - accuracy: 0.8000\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 346us/sample - loss: 0.6733 - accuracy: 0.8000\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 330us/sample - loss: 0.6708 - accuracy: 0.9000\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 362us/sample - loss: 0.6682 - accuracy: 0.9000\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 295us/sample - loss: 0.6657 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 337us/sample - loss: 0.6631 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 311us/sample - loss: 0.6606 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 251us/sample - loss: 0.6580 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 267us/sample - loss: 0.6554 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 281us/sample - loss: 0.6528 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 275us/sample - loss: 0.6502 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 275us/sample - loss: 0.6476 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 350us/sample - loss: 0.6449 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 318us/sample - loss: 0.6423 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf821496a0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_x, _class, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/1 [============================================================================================================================================================================================================================================================================================================] - 0s 7ms/sample - loss: 0.6396 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(padded_x, _class)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
