{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 시퀀스\n",
    "\n",
    "해당 단어가 나타났을때 다음 단어를 예측하면서 흘러가는 것\n",
    "\n",
    "```\n",
    "p(사건|조건) = p(조건, 사건) == `<모델>` / p(조건) \n",
    "```\n",
    "\n",
    "으로 출현 확률을 계산하는데 이걸 문장 길이에 따라서 n 번 반복\n",
    "\n",
    "ex)\n",
    "\n",
    "```\n",
    "p(운전|난폭) = p(난폭, 운전) / p(난폭)\n",
    "```\n",
    "\n",
    "일반적인 좌 -> 우 방향으로 다음 단어 예측을 순방향, 반대는 역방향 언어 모델\n",
    "\n",
    "p(w|context `주변 맥락 정보`) 로 사용하기도 함\n",
    "\n",
    "- 마스크 언어 모델\n",
    "\n",
    "완전 문장 데이터에서 일부 단어를 마스킹 처리 시키면서 학습\n",
    "\n",
    "- 스킵-그램 모델\n",
    "\n",
    "중심 단어 주위의 단어를 이용해 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트랜스포머\n",
    "\n",
    "### 인코더 - 디코더\n",
    "\n",
    "기계 번역에서 주로 사용되어지는 방식\n",
    "\n",
    "입력 데이터를 단어 시퀀스를 이용해 주요 정보를 압축 한 다음 그 정보를 바탕으로 디코더에 타겟 시퀀스 데이터와 함께 전달하여 변환된 결과를 복원하는 방식\n",
    "\n",
    "ex)\n",
    "\n",
    "한글로 된\n",
    "\n",
    "```\n",
    "나는 26살 입니다.\n",
    "```\n",
    "\n",
    "를 인코더에 넣고 디코더엔 시퀀스 시작 키 `<s>` 를 넣으면 \n",
    "\n",
    "처음은 결과가 `I'm`\n",
    "\n",
    "그 다음에도 똑같이 인코더에 \n",
    "\n",
    "```\n",
    "나는 26살 입니다.\n",
    "```\n",
    "\n",
    "디코더엔 `<s> I'm` 를 넣으면 \n",
    "\n",
    "`I'm 26` 가 나오는 형식\n",
    "\n",
    "이런 과정을 반복하여 출현 확률 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 셀프 어텐션\n",
    "\n",
    "자신에게 중요한 부분을 집중하여 성능 올리는 방식\n",
    "\n",
    "|어텐션|합성곱 신경망|순환 신경망|셀프 어텐션|\n",
    "|:------:|:--------:|:------:|:---:|\n",
    "|RNN 문제점을 해결하기 위해 해당 단어가 어떤 요소에 주목할지를 결정 `카페-cafe` 이런식으로|CNN, 지역 특징 추출, 차례데로 단어들을 넘기며 학습, 대신 커널 크기만큼만 학습하는 단점| RNN, 특징중 하나가 이전에 있었던건 거의 잊어지게 되어지는 특징으로 인해 마지막 단어를 거의 의미 있다고 판단하는 경우 존재| 입력 시퀀스에서 부터 연관관계 연결을 지음 RNN 없이 동작, 디코더 블록 수 만큼 반복, `거기-카페`, `갔었어-카페`|\n",
    "\n",
    "어떤 요소에 주목해야하는지와 그 요소가 어떤 요소와 연관이 있는지도 알 수 있음\n",
    "\n",
    "query, key, value 로 이루어짐\n",
    "\n",
    "문장을 이루는 소스 값을 쿼리,\n",
    "\n",
    "그 쿼리에서 뻗어나와 같은 문장에 연결관계를 지을 타겟들을 키\n",
    "\n",
    "키 각각에 연관성 가중치를 값 으로 정의\n",
    "\n",
    "각각 계산은 행렬 수식\n",
    "\n",
    "query = (word embedding demension x word num) x W(q)\n",
    "\n",
    "key = (word embedding demension x word num) x W(k)\n",
    "\n",
    "value = (word embedding demension x word num) x W(v)\n",
    "\n",
    "W 값들은 각각 갱신되며 서로 다른 초기값들을 가짐\n",
    "\n",
    "#### Attention 계산해보기\n",
    "\n",
    "```\n",
    "Attension(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "```\n",
    "\n",
    "그니까 풀어서 보면\n",
    "\n",
    "`쿼리` 계산 값과 `전치한 키` 계산을 곱한 걸 차원 수 만큼 제곱근 한 걸로 나누어 주고 softmax 치환한 다음 `가중치` 값과 곱함\n",
    "\n",
    "QK^T\n",
    "\n",
    "이때 쿼리와 키간의 문맥적 관계성이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 2.0, 0.0, 2.0],\n",
    "    [1.0, 1.0, 1.0, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_query = torch.tensor([\n",
    "    [1.0, 0.0, 1.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_key = torch.tensor([\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_value = torch.tensor([\n",
    "    [0.0, 2.0, 0.0],\n",
    "    [0.0, 3.0, 0.0],\n",
    "    [1.0, 0.0, 3.0],\n",
    "    [1.0, 1.0, 0.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 1., 3.]]),\n",
       " tensor([[0., 1., 1.],\n",
       "         [4., 4., 0.],\n",
       "         [2., 3., 1.]]),\n",
       " tensor([[1., 2., 3.],\n",
       "         [2., 8., 0.],\n",
       "         [2., 6., 3.]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mulQuery = torch.matmul(x, w_query)\n",
    "mulKey = torch.matmul(x, w_key)\n",
    "mulValue = torch.matmul(x, w_value)\n",
    "\n",
    "mulQuery, mulKey, mulValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  4.],\n",
       "        [ 4., 16., 12.],\n",
       "        [ 4., 12., 10.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attScore = torch.matmul(mulQuery, mulKey.T)\n",
    "attScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7320508075688772, torch.Size([3, 3]), 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimSqrt = np.sqrt(mulKey.shape[-1])\n",
    "dimSqrt, mulKey.shape, mulKey.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01],\n",
       "        [8.9045e-04, 9.0884e-01, 9.0267e-02],\n",
       "        [7.4449e-03, 7.5471e-01, 2.3785e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attenSoftmax = softmax(attScore / dimSqrt, dim=-1)\n",
    "attenSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8639, 6.3194, 1.7042],\n",
       "        [1.9991, 7.8141, 0.2735],\n",
       "        [1.9926, 7.4796, 0.7359]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.matmul(attenSoftmax, mulValue)\n",
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "위의 self-attention 을 동시에 여러번 하는 것\n",
    "\n",
    "즉 같은 문서를 읽더라도 여러 다른 사람들이 각자 읽어서 연산을 진행함\n",
    "\n",
    "행렬의 크기는 \n",
    "\n",
    "`입력 단어 수` x `value 차원 수` => Z\n",
    "\n",
    "가 되어지며 이걸 여러 다른 사람들 즉 헤더들의 수 만큼 확장\n",
    "\n",
    "scaleUp(Z, `헤더 수`)\n",
    "\n",
    "여기서 임의 가중치 행렬 W 을 곱하는데 크기는 `self-attention` 행렬의 열 크기 x 목표 차원 수\n",
    "\n",
    "최종적으론 `입력 단어 수` x `목표 차원 수`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward neural network\n",
    "\n",
    "(입력 x 가중치) + ... 해당 레이어에 있는 노드 갯수 만큼 n번 반복 + 바이어스 => 계산 값을 활성함수에 전달 => 결과 값을 또 다른 레이어의 노드에 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2, 1])\n",
    "w1 = torch.tensor([[3, 2, -4], [2, -3, 1]])\n",
    "b1 = 1\n",
    "w2 = torch.tensor([[-1, 1], [1, 2], [3, 1]])\n",
    "b2 = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}2&1\\\\ \\end{bmatrix}$ x $\\begin{bmatrix}3&2&-4\\\\2&-3&1\\\\ \\end{bmatrix}$ + b1 = $\\begin{bmatrix}9&2&-6\\\\ \\end{bmatrix}$\n",
    "\n",
    "```\n",
    "=> relu =>\n",
    "```\n",
    "\n",
    "$\\begin{bmatrix}9&2&0\\\\ \\end{bmatrix}$ x $\\begin{bmatrix}-1&1\\\\1&2\\\\3&1\\\\ \\end{bmatrix}$ + b2 = $\\begin{bmatrix}-8&12\\\\ \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  2, -6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beforeRelu = torch.matmul(x, w1) + b1\n",
    "beforeRelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 2, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afterRelu = torch.nn.functional.relu(beforeRelu)\n",
    "afterRelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8, 12])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.matmul(afterRelu, w2) + b2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잔차 연결 residual connection\n",
    "\n",
    "위 같은 블록 연산을 하는 걸 외에 건너 뛰는 경로를 추가 하는 것\n",
    "\n",
    "y = F(x) 에 y = F(x) + x 와 같은 꼴\n",
    "\n",
    "연속적인 결과 값 베이스 연산이 아닌 원본 값을 활용해 가면서 여러 경우의 수로 연산이 가능해 짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 레이어 정규화\n",
    "\n",
    "값들을 너무 들쭉날쭉 하게 하지 말고 균일하게 맞추어 주는 작업\n",
    "\n",
    "(x - 평균) / 표준편차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]])\n",
    "x.shape, x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((3,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.LayerNorm(x.shape[-1])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2247,  0.0000,  1.2247],\n",
       "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = m(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight, m.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop out\n",
    "\n",
    "일부 노드를 일부로 꺼주는 작업\n",
    "\n",
    "학습이 너무 잘 되어 과적합 되어지는 문제 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Dropout(p=0.2) # 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1387, -1.0628,  2.1952,  0.7804,  0.7318,  0.4783,  1.1320,  0.2525,\n",
       "          0.0367, -0.5461]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -1.3286,  2.7440,  0.9755,  0.9148,  0.5978,  1.4150,  0.0000,\n",
       "          0.0459, -0.6827]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = m(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노드를 끄는 것만이 아닌 1 / (1 - p) 값을 곱하는 연산도 진행\n",
    "\n",
    "일반적으론 p 값을 0.1 로 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam optimizer\n",
    "\n",
    "모델과 정답 사이의 오차를 줄여주기 위한 파라미터 수정 작업 \n",
    "\n",
    "기본적으로 경사 하강법을 사용\n",
    "\n",
    "이를 최적화라고 부르며 그 방법중 Adam 이란 방식이 있음\n",
    "\n",
    "내려가는 방향은 경사가 급한 방향으로가지만 관성이 있는것 처럼 일부 방향을 유지함\n",
    "\n",
    "보폭은 처음 간 곳은 빠르게, 가본 곳은 천천히 움직임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
