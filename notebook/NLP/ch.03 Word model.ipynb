{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 시퀀스\n",
    "\n",
    "해당 단어가 나타났을때 다음 단어를 예측하면서 흘러가는 것\n",
    "\n",
    "```\n",
    "p(사건|조건) = p(조건, 사건) == `<모델>` / p(조건) \n",
    "```\n",
    "\n",
    "으로 출현 확률을 계산하는데 이걸 문장 길이에 따라서 n 번 반복\n",
    "\n",
    "ex)\n",
    "\n",
    "```\n",
    "p(운전|난폭) = p(난폭, 운전) / p(난폭)\n",
    "```\n",
    "\n",
    "일반적인 좌 -> 우 방향으로 다음 단어 예측을 순방향, 반대는 역방향 언어 모델\n",
    "\n",
    "p(w|context `주변 맥락 정보`) 로 사용하기도 함\n",
    "\n",
    "- 마스크 언어 모델\n",
    "\n",
    "완전 문장 데이터에서 일부 단어를 마스킹 처리 시키면서 학습\n",
    "\n",
    "- 스킵-그램 모델\n",
    "\n",
    "중심 단어 주위의 단어를 이용해 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트랜스포머\n",
    "\n",
    "### 인코더 - 디코더\n",
    "\n",
    "기계 번역에서 주로 사용되어지는 방식\n",
    "\n",
    "입력 데이터를 단어 시퀀스를 이용해 주요 정보를 압축 한 다음 그 정보를 바탕으로 디코더에 타겟 시퀀스 데이터와 함께 전달하여 변환된 결과를 복원하는 방식\n",
    "\n",
    "ex)\n",
    "\n",
    "한글로 된\n",
    "\n",
    "```\n",
    "나는 26살 입니다.\n",
    "```\n",
    "\n",
    "를 인코더에 넣고 디코더엔 시퀀스 시작 키 `<s>` 를 넣으면 \n",
    "\n",
    "처음은 결과가 `I'm`\n",
    "\n",
    "그 다음에도 똑같이 인코더에 \n",
    "\n",
    "```\n",
    "나는 26살 입니다.\n",
    "```\n",
    "\n",
    "디코더엔 `<s> I'm` 를 넣으면 \n",
    "\n",
    "`I'm 26` 가 나오는 형식\n",
    "\n",
    "이런 과정을 반복하여 출현 확률 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 셀프 어텐션\n",
    "\n",
    "자신에게 중요한 부분을 집중하여 성능 올리는 방식\n",
    "\n",
    "|어텐션|합성곱 신경망|순환 신경망|셀프 어텐션|\n",
    "|:------:|:--------:|:------:|:---:|\n",
    "|RNN 문제점을 해결하기 위해 해당 단어가 어떤 요소에 주목할지를 결정 `카페-cafe` 이런식으로|CNN, 지역 특징 추출, 차례데로 단어들을 넘기며 학습, 대신 커널 크기만큼만 학습하는 단점| RNN, 특징중 하나가 이전에 있었던건 거의 잊어지게 되어지는 특징으로 인해 마지막 단어를 거의 의미 있다고 판단하는 경우 존재| 입력 시퀀스에서 부터 연관관계 연결을 지음 RNN 없이 동작, 디코더 블록 수 만큼 반복, `거기-카페`, `갔었어-카페`|\n",
    "\n",
    "어떤 요소에 주목해야하는지와 그 요소가 어떤 요소와 연관이 있는지도 알 수 있음\n",
    "\n",
    "query, key, value 로 이루어짐\n",
    "\n",
    "문장을 이루는 소스 값을 쿼리,\n",
    "\n",
    "그 쿼리에서 뻗어나와 같은 문장에 연결관계를 지을 타겟들을 키\n",
    "\n",
    "키 각각에 연관성 가중치를 값 으로 정의\n",
    "\n",
    "각각 계산은 행렬 수식\n",
    "\n",
    "query = (word embedding demension x word num) x W(q)\n",
    "\n",
    "key = (word embedding demension x word num) x W(k)\n",
    "\n",
    "value = (word embedding demension x word num) x W(v)\n",
    "\n",
    "W 값들은 각각 갱신되며 서로 다른 초기값들을 가짐\n",
    "\n",
    "#### Attention 계산해보기\n",
    "\n",
    "```\n",
    "Attension(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "```\n",
    "\n",
    "그니까 풀어서 보면\n",
    "\n",
    "`쿼리` 계산 값과 `전치한 키` 계산을 곱한 걸 차원 수 만큼 제곱근 한 걸로 나누어 주고 softmax 치환한 다음 `가중치` 값과 곱함\n",
    "\n",
    "QK^T\n",
    "\n",
    "이때 쿼리와 키간의 문맥적 관계성이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 2.0, 0.0, 2.0],\n",
    "    [1.0, 1.0, 1.0, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_query = torch.tensor([\n",
    "    [1.0, 0.0, 1.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_key = torch.tensor([\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_value = torch.tensor([\n",
    "    [0.0, 2.0, 0.0],\n",
    "    [0.0, 3.0, 0.0],\n",
    "    [1.0, 0.0, 3.0],\n",
    "    [1.0, 1.0, 0.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 1., 3.]]),\n",
       " tensor([[0., 1., 1.],\n",
       "         [4., 4., 0.],\n",
       "         [2., 3., 1.]]),\n",
       " tensor([[1., 2., 3.],\n",
       "         [2., 8., 0.],\n",
       "         [2., 6., 3.]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mulQuery = torch.matmul(x, w_query)\n",
    "mulKey = torch.matmul(x, w_key)\n",
    "mulValue = torch.matmul(x, w_value)\n",
    "\n",
    "mulQuery, mulKey, mulValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  4.],\n",
       "        [ 4., 16., 12.],\n",
       "        [ 4., 12., 10.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attScore = torch.matmul(mulQuery, mulKey.T)\n",
    "attScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7320508075688772, torch.Size([3, 3]), 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimSqrt = np.sqrt(mulKey.shape[-1])\n",
    "dimSqrt, mulKey.shape, mulKey.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01],\n",
       "        [8.9045e-04, 9.0884e-01, 9.0267e-02],\n",
       "        [7.4449e-03, 7.5471e-01, 2.3785e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attenSoftmax = softmax(attScore / dimSqrt, dim=-1)\n",
    "attenSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8639, 6.3194, 1.7042],\n",
       "        [1.9991, 7.8141, 0.2735],\n",
       "        [1.9926, 7.4796, 0.7359]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.matmul(attenSoftmax, mulValue)\n",
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "위의 self-attention 을 동시에 여러번 하는 것\n",
    "\n",
    "즉 같은 문서를 읽더라도 여러 다른 사람들이 각자 읽어서 연산을 진행함\n",
    "\n",
    "행렬의 크기는 \n",
    "\n",
    "`입력 단어 수` x `value 차원 수` => Z\n",
    "\n",
    "가 되어지며 이걸 여러 다른 사람들 즉 헤더들의 수 만큼 확장\n",
    "\n",
    "scaleUp(Z, `헤더 수`)\n",
    "\n",
    "여기서 임의 가중치 행렬 W 을 곱하는데 크기는 `self-attention` 행렬의 열 크기 x 목표 차원 수\n",
    "\n",
    "최종적으론 `입력 단어 수` x `목표 차원 수`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward neural network\n",
    "\n",
    "(입력 x 가중치) + ... 해당 레이어에 있는 노드 갯수 만큼 n번 반복 + 바이어스 => 계산 값을 활성함수에 전달 => 결과 값을 또 다른 레이어의 노드에 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2, 1])\n",
    "w1 = torch.tensor([[3, 2, -4], [2, -3, 1]])\n",
    "b1 = 1\n",
    "w2 = torch.tensor([[-1, 1], [1, 2], [3, 1]])\n",
    "b2 = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}2&1\\\\ \\end{bmatrix}$ x $\\begin{bmatrix}3&2&-4\\\\2&-3&1\\\\ \\end{bmatrix}$ + b1 = $\\begin{bmatrix}9&2&-6\\\\ \\end{bmatrix}$\n",
    "\n",
    "```\n",
    "=> relu =>\n",
    "```\n",
    "\n",
    "$\\begin{bmatrix}9&2&0\\\\ \\end{bmatrix}$ x $\\begin{bmatrix}-1&1\\\\1&2\\\\3&1\\\\ \\end{bmatrix}$ + b2 = $\\begin{bmatrix}-8&12\\\\ \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  2, -6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beforeRelu = torch.matmul(x, w1) + b1\n",
    "beforeRelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 2, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afterRelu = torch.nn.functional.relu(beforeRelu)\n",
    "afterRelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8, 12])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.matmul(afterRelu, w2) + b2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잔차 연결 residual connection\n",
    "\n",
    "위 같은 블록 연산을 하는 걸 외에 건너 뛰는 경로를 추가 하는 것\n",
    "\n",
    "y = F(x) 에 y = F(x) + x 와 같은 꼴\n",
    "\n",
    "연속적인 결과 값 베이스 연산이 아닌 원본 값을 활용해 가면서 여러 경우의 수로 연산이 가능해 짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 레이어 정규화\n",
    "\n",
    "값들을 너무 들쭉날쭉 하게 하지 말고 균일하게 맞추어 주는 작업\n",
    "\n",
    "(x - 평균) / 표준편차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]])\n",
    "x.shape, x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((3,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.LayerNorm(x.shape[-1])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2247,  0.0000,  1.2247],\n",
       "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = m(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight, m.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop out\n",
    "\n",
    "일부 노드를 일부로 꺼주는 작업\n",
    "\n",
    "학습이 너무 잘 되어 과적합 되어지는 문제 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Dropout(p=0.2) # 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1387, -1.0628,  2.1952,  0.7804,  0.7318,  0.4783,  1.1320,  0.2525,\n",
       "          0.0367, -0.5461]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -1.3286,  2.7440,  0.9755,  0.9148,  0.5978,  1.4150,  0.0000,\n",
       "          0.0459, -0.6827]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = m(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노드를 끄는 것만이 아닌 1 / (1 - p) 값을 곱하는 연산도 진행\n",
    "\n",
    "일반적으론 p 값을 0.1 로 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam optimizer\n",
    "\n",
    "모델과 정답 사이의 오차를 줄여주기 위한 파라미터 수정 작업 \n",
    "\n",
    "기본적으로 경사 하강법을 사용\n",
    "\n",
    "이를 최적화라고 부르며 그 방법중 Adam 이란 방식이 있음\n",
    "\n",
    "내려가는 방향은 경사가 급한 방향으로가지만 관성이 있는것 처럼 일부 방향을 유지함\n",
    "\n",
    "보폭은 처음 간 곳은 빠르게, 가본 곳은 천천히 움직임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|GPT | BERT |\n",
    "|:--:|:----:|\n",
    "|과거 단어들을 이용해 다음 단어 예측| 문장 중간에 마스킹처리를 한 다음 적절한 단어를 예측|\n",
    "|단방향|양방향|\n",
    "|문장 생성|의미 추출|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파인 튜닝\n",
    "\n",
    "다운스트림(미리 업스트림에서 학습된 내용 활용) 데이터 전체 활용하여 전체 모델 업데이트\n",
    "\n",
    "- 프롬포트 튜닝: 데이터 전체 활용, 일부 모델 업데이트\n",
    "- 인컨텍스트 튜닝: 데이터 일부 사용, 모델 업데이트 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1931e2b25f43eaac3ffde5011fb503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/244k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6cd5bc55c84738ab7c585a36a39d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85aa4003ef0341e2b56c4e4f2d7d3d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://github.com/Beomi/KcBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"beomi/kcbert-base\",\n",
    "    do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = BertConfig.from_pretrained(\n",
    "    \"beomi/kcbert-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcb18c6211d4d27ac190c2abec00ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\n",
    "    \"beomi/kcbert-base\",\n",
    "    config=model_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 300,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"안녕하세요\", \"하이\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 19017, 8482, 3, 0, 0, 0, 0, 0, 0], [2, 15830, 3, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tokenizer(\n",
    "    sentences,\n",
    "    max_length=10,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True # 길이 범위 초과시 자름\n",
    ")\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_ids 에서 `2`, `3` 은 각각 CLS, SEP 라는 스페셜 토큰임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 19017,  8482,     3,     0,     0,     0,     0,     0,     0],\n",
       "         [    2, 15830,     3,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tensor = {k: torch.tensor(v) for k, v in features.items()}\n",
    "features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('input_ids', [[2, 19017, 8482, 3, 0, 0, 0, 0, 0, 0], [2, 15830, 3, 0, 0, 0, 0, 0, 0, 0]]), ('token_type_ids', [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), ('attention_mask', [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: input_ids v: [[2, 19017, 8482, 3, 0, 0, 0, 0, 0, 0], [2, 15830, 3, 0, 0, 0, 0, 0, 0, 0]]\n",
      "k: token_type_ids v: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "k: attention_mask v: [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "for k, v in features.items():\n",
    "    print(\"k:\", k, \"v:\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.6969, -0.8248,  1.7512,  ..., -0.3732,  0.7399,  1.1907],\n",
       "         [-1.4803, -0.4398,  0.9444,  ..., -0.7405, -0.0211,  1.3064],\n",
       "         [-1.4299, -0.5033, -0.2069,  ...,  0.1285, -0.2611,  1.6057],\n",
       "         ...,\n",
       "         [-1.4406,  0.3431,  1.4043,  ..., -0.0565,  0.8450, -0.2170],\n",
       "         [-1.3625, -0.2404,  1.1757,  ...,  0.8876, -0.1054,  0.0734],\n",
       "         [-1.4244,  0.1518,  1.2920,  ...,  0.0245,  0.7572,  0.0080]],\n",
       "\n",
       "        [[ 0.7565, -1.7148,  2.1903,  ..., -0.4291,  0.9469,  0.7394],\n",
       "         [ 1.1048, -1.3408,  1.9444,  ...,  0.3340,  0.2276, -0.5220],\n",
       "         [-0.1317, -0.6982,  1.3086,  ...,  0.4448,  0.0836, -0.6280],\n",
       "         ...,\n",
       "         [ 0.0666, -0.7225,  1.8425,  ...,  1.2435,  1.6184,  0.1339],\n",
       "         [ 0.1781, -0.7823,  1.6892,  ...,  1.1230,  1.5262,  0.1855],\n",
       "         [-0.0183, -0.8602,  2.5182,  ...,  0.9201,  0.3697, -0.7420]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.1594,  0.0547,  0.1101,  ...,  0.2684,  0.1596, -0.9828],\n",
       "        [-0.9349,  0.1990, -0.0390,  ...,  0.5419,  0.0375, -0.9960]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**features_tensor)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 2개의 시퀀스 길이 10, 768차원 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
