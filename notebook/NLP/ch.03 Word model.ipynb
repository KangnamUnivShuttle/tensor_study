{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 시퀀스\n",
    "\n",
    "해당 단어가 나타났을때 다음 단어를 예측하면서 흘러가는 것\n",
    "\n",
    "```\n",
    "p(사건|조건) = p(조건, 사건) == `<모델>` / p(조건) \n",
    "```\n",
    "\n",
    "으로 출현 확률을 계산하는데 이걸 문장 길이에 따라서 n 번 반복\n",
    "\n",
    "ex)\n",
    "\n",
    "```\n",
    "p(운전|난폭) = p(난폭, 운전) / p(난폭)\n",
    "```\n",
    "\n",
    "일반적인 좌 -> 우 방향으로 다음 단어 예측을 순방향, 반대는 역방향 언어 모델\n",
    "\n",
    "p(w|context `주변 맥락 정보`) 로 사용하기도 함\n",
    "\n",
    "- 마스크 언어 모델\n",
    "\n",
    "완전 문장 데이터에서 일부 단어를 마스킹 처리 시키면서 학습\n",
    "\n",
    "- 스킵-그램 모델\n",
    "\n",
    "중심 단어 주위의 단어를 이용해 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트랜스포머\n",
    "\n",
    "### 인코더 - 디코더\n",
    "\n",
    "기계 번역에서 주로 사용되어지는 방식\n",
    "\n",
    "입력 데이터를 단어 시퀀스를 이용해 주요 정보를 압축 한 다음 그 정보를 바탕으로 디코더에 타겟 시퀀스 데이터와 함께 전달하여 변환된 결과를 복원하는 방식\n",
    "\n",
    "ex)\n",
    "\n",
    "한글로 된\n",
    "\n",
    "```\n",
    "나는 26살 입니다.\n",
    "```\n",
    "\n",
    "를 인코더에 넣고 디코더엔 시퀀스 시작 키 `<s>` 를 넣으면 \n",
    "\n",
    "처음은 결과가 `I'm`\n",
    "\n",
    "그 다음에도 똑같이 인코더에 \n",
    "\n",
    "```\n",
    "나는 26살 입니다.\n",
    "```\n",
    "\n",
    "디코더엔 `<s> I'm` 를 넣으면 \n",
    "\n",
    "`I'm 26` 가 나오는 형식\n",
    "\n",
    "이런 과정을 반복하여 출현 확률 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 셀프 어텐션\n",
    "\n",
    "자신에게 중요한 부분을 집중하여 성능 올리는 방식\n",
    "\n",
    "|어텐션|합성곱 신경망|순환 신경망|셀프 어텐션|\n",
    "|:------:|:--------:|:------:|:---:|\n",
    "|RNN 문제점을 해결하기 위해 해당 단어가 어떤 요소에 주목할지를 결정 `카페-cafe` 이런식으로|CNN, 지역 특징 추출, 차례데로 단어들을 넘기며 학습, 대신 커널 크기만큼만 학습하는 단점| RNN, 특징중 하나가 이전에 있었던건 거의 잊어지게 되어지는 특징으로 인해 마지막 단어를 거의 의미 있다고 판단하는 경우 존재| 입력 시퀀스에서 부터 연관관계 연결을 지음 RNN 없이 동작, 디코더 블록 수 만큼 반복, `거기-카페`, `갔었어-카페`|\n",
    "\n",
    "어떤 요소에 주목해야하는지와 그 요소가 어떤 요소와 연관이 있는지도 알 수 있음\n",
    "\n",
    "query, key, value 로 이루어짐\n",
    "\n",
    "문장을 이루는 소스 값을 쿼리,\n",
    "\n",
    "그 쿼리에서 뻗어나와 같은 문장에 연결관계를 지을 타겟들을 키\n",
    "\n",
    "키 각각에 연관성 가중치를 값 으로 정의\n",
    "\n",
    "각각 계산은 행렬 수식\n",
    "\n",
    "query = (word embedding demension x word num) x W(q)\n",
    "\n",
    "key = (word embedding demension x word num) x W(k)\n",
    "\n",
    "value = (word embedding demension x word num) x W(v)\n",
    "\n",
    "W 값들은 각각 갱신되며 서로 다른 초기값들을 가짐\n",
    "\n",
    "#### Attention 계산해보기\n",
    "\n",
    "```\n",
    "Attension(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "```\n",
    "\n",
    "그니까 풀어서 보면\n",
    "\n",
    "`쿼리` 계산 값과 `전치한 키` 계산을 곱한 걸 차원 수 만큼 제곱근 한 걸로 나누어 주고 softmax 치환한 다음 `가중치` 값과 곱함\n",
    "\n",
    "QK^T\n",
    "\n",
    "이때 쿼리와 키간의 문맥적 관계성이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 2.0, 0.0, 2.0],\n",
    "    [1.0, 1.0, 1.0, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_query = torch.tensor([\n",
    "    [1.0, 0.0, 1.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_key = torch.tensor([\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_value = torch.tensor([\n",
    "    [0.0, 2.0, 0.0],\n",
    "    [0.0, 3.0, 0.0],\n",
    "    [1.0, 0.0, 3.0],\n",
    "    [1.0, 1.0, 0.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 1., 3.]]),\n",
       " tensor([[0., 1., 1.],\n",
       "         [4., 4., 0.],\n",
       "         [2., 3., 1.]]),\n",
       " tensor([[1., 2., 3.],\n",
       "         [2., 8., 0.],\n",
       "         [2., 6., 3.]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mulQuery = torch.matmul(x, w_query)\n",
    "mulKey = torch.matmul(x, w_key)\n",
    "mulValue = torch.matmul(x, w_value)\n",
    "\n",
    "mulQuery, mulKey, mulValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  4.],\n",
       "        [ 4., 16., 12.],\n",
       "        [ 4., 12., 10.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attScore = torch.matmul(mulQuery, mulKey.T)\n",
    "attScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7320508075688772, torch.Size([3, 3]), 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimSqrt = np.sqrt(mulKey.shape[-1])\n",
    "dimSqrt, mulKey.shape, mulKey.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01],\n",
       "        [8.9045e-04, 9.0884e-01, 9.0267e-02],\n",
       "        [7.4449e-03, 7.5471e-01, 2.3785e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attenSoftmax = softmax(attScore / dimSqrt, dim=-1)\n",
    "attenSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8639, 6.3194, 1.7042],\n",
       "        [1.9991, 7.8141, 0.2735],\n",
       "        [1.9926, 7.4796, 0.7359]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.matmul(attenSoftmax, mulValue)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
